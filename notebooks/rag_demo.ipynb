{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PaperBot RAG Demo\n",
        "\n",
        "This notebook demonstrates:\n",
        "1. Embedding model training/loading\n",
        "2. Document chunking and embedding\n",
        "3. Vector search and retrieval\n",
        "4. RAG-based Q/A with citations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path().resolve().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'paperbot.settings')\n",
        "import django\n",
        "django.setup()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Embedding Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load embedding model\n",
        "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "embedding_model = SentenceTransformer(model_name)\n",
        "\n",
        "print(f\"Model loaded: {model_name}\")\n",
        "print(f\"Embedding dimension: {embedding_model.get_sentence_embedding_dimension()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Sample Document Processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample research paper text (in production, this comes from PDF extraction)\n",
        "sample_text = \"\"\"\n",
        "Machine Learning for Document Understanding\n",
        "\n",
        "Abstract: This paper presents a novel approach to document understanding using \n",
        "machine learning techniques. We propose a transformer-based architecture that \n",
        "can extract structured information from unstructured documents.\n",
        "\n",
        "1. Introduction\n",
        "Document understanding is a critical task in information retrieval systems. \n",
        "Traditional methods rely on rule-based extraction, which is brittle and \n",
        "does not scale well.\n",
        "\n",
        "2. Methodology\n",
        "Our approach uses a pre-trained transformer model fine-tuned on document \n",
        "understanding tasks. We employ a two-stage process: first, we extract text \n",
        "from PDFs using OCR, then we apply semantic embeddings to enable similarity search.\n",
        "\n",
        "3. Results\n",
        "We evaluated our system on a dataset of 10,000 research papers. The system \n",
        "achieved 95% accuracy in information extraction tasks.\n",
        "\n",
        "4. Related Work\n",
        "Previous work in document understanding includes BERT-based models (Devlin et al., 2019) \n",
        "and GPT-based approaches (Brown et al., 2020). Our method builds upon these \n",
        "foundations while introducing novel improvements.\n",
        "\n",
        "5. Conclusion\n",
        "We have demonstrated that transformer-based models can effectively understand \n",
        "and extract information from documents. Future work will focus on multi-modal \n",
        "understanding incorporating images and tables.\n",
        "\"\"\"\n",
        "\n",
        "print(f\"Sample text length: {len(sample_text)} characters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from api.utils import PDFProcessor\n",
        "\n",
        "# Chunk the text\n",
        "chunks = PDFProcessor.chunk_text(sample_text, chunk_size=200, overlap=50)\n",
        "\n",
        "print(f\"Number of chunks: {len(chunks)}\")\n",
        "print(\"\\nFirst chunk:\")\n",
        "print(chunks[0]['text'][:200] + \"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Create Embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create embeddings for all chunks\n",
        "chunk_texts = [chunk['text'] for chunk in chunks]\n",
        "embeddings = embedding_model.encode(chunk_texts, convert_to_numpy=True)\n",
        "\n",
        "print(f\"Embeddings shape: {embeddings.shape}\")\n",
        "print(f\"Number of chunks: {len(chunk_texts)}\")\n",
        "print(f\"Embedding dimension: {embeddings.shape[1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Build Vector Index (FAISS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import faiss\n",
        "\n",
        "# Create FAISS index\n",
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "# Add embeddings to index\n",
        "embeddings_float32 = embeddings.astype('float32')\n",
        "index.add(embeddings_float32)\n",
        "\n",
        "print(f\"Index size: {index.ntotal}\")\n",
        "print(f\"Index dimension: {index.d}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Query and Retrieve\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example query\n",
        "query = \"What is the methodology used in this paper?\"\n",
        "\n",
        "# Create query embedding\n",
        "query_embedding = embedding_model.encode([query], convert_to_numpy=True).astype('float32')\n",
        "\n",
        "# Search for similar chunks\n",
        "k = 3  # Top 3 results\n",
        "distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"\\nTop {k} similar chunks:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, (idx, dist) in enumerate(zip(indices[0], distances[0])):\n",
        "    print(f\"\\nResult {i+1} (distance: {dist:.4f}):\")\n",
        "    print(f\"Chunk {idx}:\")\n",
        "    print(chunks[idx]['text'][:300] + \"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. RAG Prompt Construction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build RAG prompt with retrieved context\n",
        "retrieved_chunks = [chunks[idx] for idx in indices[0]]\n",
        "\n",
        "context = \"\\n\\n\".join([\n",
        "    f\"[Chunk {i+1}]\\n{chunk['text']}\" \n",
        "    for i, chunk in enumerate(retrieved_chunks)\n",
        "])\n",
        "\n",
        "prompt = f\"\"\"You are a research assistant. Answer the question based on the provided context.\n",
        "Always cite which chunk you're referencing.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "print(\"RAG Prompt:\")\n",
        "print(\"=\" * 80)\n",
        "print(prompt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Simulate LLM Response (with Citations)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulated LLM response (in production, this calls OpenAI/Anthropic)\n",
        "answer = \"\"\"\n",
        "The methodology used in this paper involves a two-stage process:\n",
        "\n",
        "1. Text Extraction: First, text is extracted from PDFs using OCR technology.\n",
        "   [Reference: Chunk 2]\n",
        "\n",
        "2. Semantic Embeddings: Then, semantic embeddings are applied to enable \n",
        "   similarity search and retrieval. [Reference: Chunk 2]\n",
        "\n",
        "The approach uses a pre-trained transformer model that is fine-tuned on \n",
        "document understanding tasks. [Reference: Chunk 2]\n",
        "\"\"\"\n",
        "\n",
        "print(\"Generated Answer with Citations:\")\n",
        "print(\"=\" * 80)\n",
        "print(answer)\n",
        "\n",
        "# Extract citations\n",
        "citations = [\n",
        "    {\n",
        "        'chunk_id': idx,\n",
        "        'snippet': chunks[idx]['text'][:200],\n",
        "        'score': float(dist)\n",
        "    }\n",
        "    for idx, dist in zip(indices[0], distances[0])\n",
        "]\n",
        "\n",
        "print(\"\\n\\nCitations:\")\n",
        "for i, cite in enumerate(citations, 1):\n",
        "    print(f\"\\nCitation {i}:\")\n",
        "    print(f\"  Chunk ID: {cite['chunk_id']}\")\n",
        "    print(f\"  Score: {cite['score']:.4f}\")\n",
        "    print(f\"  Snippet: {cite['snippet'][:150]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Integration with Django Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from core.models import Document, Chunk, ChunkEmbedding, EmbeddingModel\n",
        "from django.contrib.auth import get_user_model\n",
        "\n",
        "User = get_user_model()\n",
        "\n",
        "# Example: Get a document and its chunks\n",
        "try:\n",
        "    doc = Document.objects.filter(status='indexed').first()\n",
        "    if doc:\n",
        "        print(f\"Document: {doc.title}\")\n",
        "        print(f\"Status: {doc.status}\")\n",
        "        print(f\"Chunks: {doc.chunks.count()}\")\n",
        "        \n",
        "        # Get embeddings\n",
        "        embeddings_count = ChunkEmbedding.objects.filter(chunk__document=doc).count()\n",
        "        print(f\"Embeddings: {embeddings_count}\")\n",
        "    else:\n",
        "        print(\"No indexed documents found. Upload a document first.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Summary\n",
        "\n",
        "This notebook demonstrated:\n",
        "1. ✅ Loading embedding models\n",
        "2. ✅ Text chunking\n",
        "3. ✅ Creating embeddings\n",
        "4. ✅ Building vector index (FAISS)\n",
        "5. ✅ Querying and retrieval\n",
        "6. ✅ RAG prompt construction\n",
        "7. ✅ Citation extraction\n",
        "\n",
        "In production, these steps are automated through:\n",
        "- Celery tasks for async processing\n",
        "- Django models for persistence\n",
        "- API endpoints for Q/A and summarization\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
